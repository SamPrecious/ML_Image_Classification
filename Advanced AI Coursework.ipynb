{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Images as Birds or Airplanes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset:** https://www.cs.toronto.edu/~kriz/cifar.html<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More writeup stuff.\n",
    "Must include learning objectives, explainations of similar work, how my work differs etc. \n",
    "probably do an abstract to explain this project as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch for building and training the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Subset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy for data manipulation and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and transforming the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we can define a transform for the image data we are about to import, turning them into tensors. We do this so [INSERT REASON HERE]. This also has the benefit of automatically normalising us from a range from 0-255, which while not necessary for image data, is worth doing for advantages such as faster training.\n",
    "[SOURCE: https://machinelearningmastery.com/how-to-manually-scale-image-pixel-data-for-deep-learning/]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we load the train and test dataset from the CIFAR-10, while also applying the transform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can view the properties of the datasets we have imported:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-set size: 50000\n",
      "Test-set size: 10000\n",
      "Classes:  ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
     ]
    }
   ],
   "source": [
    "print(\"Train-set size:\", len(trainset))\n",
    "print(\"Test-set size:\", len(testset))\n",
    "print(\"Classes: \", trainset.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the CIFAR-10 dataset has already sorted the dataset into training and testing automatically we do not need to worry about this. It has a test to train ratio of 5:1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, this dataset currently consists of 10 seperate classes, while we only want to classify between airplanes and birds. For this reason, we will filter out the other classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0 and 2 correspond to the index of the 'airplane' and 'bird' classes\n",
    "filtered_classes = [0, 2]\n",
    "\n",
    "train_indices = [i for i, label in enumerate(trainset.targets) if label in filtered_classes]\n",
    "test_indices = [i for i, label in enumerate(testset.targets) if label in filtered_classes]\n",
    "\n",
    "trainset_filtered = torch.utils.data.Subset(trainset, train_indices)\n",
    "testset_filtered = torch.utils.data.Subset(testset, test_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon filtering the dataset, we can view the properties of the new subset below. Notice how the train and test sets have shrunk to 20% of the original size, as we have removed the 80% we are not using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-set size: 10000\n",
      "Test-set size: 2000\n",
      "['airplane', 'bird']\n"
     ]
    }
   ],
   "source": [
    "print(\"Train-set size:\", len(trainset_filtered))\n",
    "print(\"Test-set size:\", len(testset_filtered))\n",
    "print([trainset.classes[i] for i in filtered_classes])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can observe the images and labels in this dataset to ensure that it is working as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bird label:  2\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plane label:  0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#1st bird in the data set\n",
    "bird_image, bird_label = trainset_filtered[0]\n",
    "print(\"Bird label: \", bird_label)\n",
    "plt.imshow(bird_image.permute(1,2,0))\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "#1st plane in the dataset\n",
    "plane_image, plane_label = trainset_filtered[4]\n",
    "print(\"Plane label: \", plane_label)\n",
    "plt.imshow(plane_image.permute(1,2,0))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see label 2 is linked with the bird label, and label 0 is linked with the planes image, as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we must create dataloaders, so we can have an iterable over our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset_filtered, batch_size=4, shuffle=True, num_workers=4)\n",
    "testloader = torch.utils.data.DataLoader(testset_filtered, batch_size=4, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should also have class labels for our invididual classes to be distinguished. In this case, it is 'bird' and 'airplane' which can be configured as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {'bird', 'airplane'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shuffle the data in our training dataset. This is to make sure that the model doesn't learn anything from the order of images input. This isn't necessary with our testing dataset however, so we can leave this false. \n",
    "Num_workers relates to the number seperate workers that will load the data in parallel. While a high number of workers may speed up performance, it may also increase memory utilisation so its good to strike a balance.\n",
    "We are also using a batch size of 4. A larger batch size can be good for speed, but too high and we risk lower accuracy and overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (x_train, y_train) in enumerate(trainset_filtered):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing neural net stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = nn.Conv2d(3,6,4,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 32, 32])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x_train.view(1,3,32,32)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = F.relu(conv1(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 29, 29])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = F.max_pool2d(x, 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 14, 14])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2 = nn.Conv2d(6,16,4,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = F.relu(conv2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 11, 11])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 5, 5])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = F.max_pool2d(x, 2, 2)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we must define a model, which we are doing with a Convolutional Neural Network as these are very good for classifying images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A convolution neural network has many different layers. These include, convolutional, pooling and fully connected layers that take the following parameters:\n",
    "Convolutional(\n",
    "    Input\n",
    "    Output\n",
    "    Kernel Size\n",
    "    Stride\n",
    "    )\n",
    "Pooling(\n",
    "    X\n",
    "    Y\n",
    "    Z)\n",
    "Fully Connected Layer(\n",
    "    Kernel Size\n",
    "    Stride\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than redefining our pooling layer, as we want one of the same size so we can just call it twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=800, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        #Convoliton layers\n",
    "        self.conv1 = nn.Conv2d(3,16,5,1)\n",
    "        self.conv2 = nn.Conv2d(16,32,5,1)\n",
    "        #Pooling layer\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        #Fully connected layers\n",
    "        self.fc1 = nn.Linear(32 * 5 * 5, 120) #32*5*5 output channel * batch size * batch size\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84,2) #2 outputs, for our 2 classifiers. Bird and Airplane\n",
    "\n",
    "    #Progress across model    \n",
    "    def forward(self, x):\n",
    "        print(\"x size:\", x.size())\n",
    "        x = nn.functional.relu(self.conv1(x))\n",
    "        print(\"x size:\", x.size())\n",
    "        x = self.pool(x)\n",
    "        print(\"x size:\", x.size())\n",
    "        x = nn.functional.relu(self.conv2(x))\n",
    "        print(\"x size:\", x.size())\n",
    "\n",
    "        x = x.view(-1, 32*5*5) #Total number of elements in tensor\n",
    "        print(\"x size after resize:\", x.size())\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        print(\"x size:\", x.size())\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        print(\"x size:\", x.size())\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = Model()\n",
    "model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our first convolution layer, we have an input of 3, as each image has 3 seperate inputs for different RGB channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Model Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explaination about hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function is what we use to calculate the error, quantifying the difference between the predicted output and the actual output. It helps us measure the performance of a model, while also indicating the direction to improve in.\n",
    "For general image classification, you will likely want to use Cross-Entropy classification, however in our case as we only have 2 sets of images, we can use Binary Cross-Entrophy as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x size: torch.Size([4, 3, 32, 32])\n",
      "x size: torch.Size([4, 16, 28, 28])\n",
      "x size: torch.Size([4, 16, 14, 14])\n",
      "x size: torch.Size([4, 32, 10, 10])\n",
      "x size after resize: torch.Size([16, 800])\n",
      "x size: torch.Size([16, 120])\n",
      "x size: torch.Size([16, 84])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([4])) that is different to the input size (torch.Size([16, 2])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[152], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, labels)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal loss for this batch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(loss\u001b[38;5;241m.\u001b[39mitem()))\n",
      "File \u001b[1;32mc:\\Users\\zambl\\miniconda3\\envs\\coursework-ai-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\zambl\\miniconda3\\envs\\coursework-ai-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\zambl\\miniconda3\\envs\\coursework-ai-env\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:618\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    617\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 618\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mbinary_cross_entropy(\u001b[38;5;28minput\u001b[39m, target, weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction)\n",
      "File \u001b[1;32mc:\\Users\\zambl\\miniconda3\\envs\\coursework-ai-env\\Lib\\site-packages\\torch\\nn\\functional.py:3118\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3116\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[0;32m   3117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize():\n\u001b[1;32m-> 3118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3119\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing a target size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) that is different to the input size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) is deprecated. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3120\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure they have the same size.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(target\u001b[38;5;241m.\u001b[39msize(), \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m   3121\u001b[0m     )\n\u001b[0;32m   3123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3124\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n",
      "\u001b[1;31mValueError\u001b[0m: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([16, 2])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "loss_fn = nn.BCELoss()\n",
    "for inputs, labels in trainloader:\n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = loss_fn(outputs, labels)\n",
    "    print('Total loss for this batch {}'.format(loss.item()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the optimiser, we will use the ADAM optimiser from PyTorch as it is a good, common optimiser for deep neural networks. [SOURCE: https://visualstudiomagazine.com/articles/2022/10/14/binary-classification-using-pytorch-2.aspx#:~:text=The%20two%20most%20common%20are,everything%20about%20every%20optimization%20algorithm.] We must also designate a learning rate. The smaller the learning rate, the slower it will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = torch.optim.Adam(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
